{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6ebf7bb",
   "metadata": {},
   "source": [
    "# Predicción Bancarrota de Empresas\n",
    "## Análisis de 6.800 empresas\n",
    "\n",
    "El siguiente conjunto de datos son **6.800 entradas de empresas de Taiwan con sus características distribuidas en 95 columnas**. De ellas 220 entraron en bancarrota de modo que el **objetivo** de este proyecto es **predecir si una empresa se ira a la bancarrota o no**, siendo las columnas sus variables independientes de las que dependa la predicción.\n",
    "\n",
    "link data: https://www.kaggle.com/datasets/fedesoriano/company-bankruptcy-prediction\n",
    "\n",
    "Para realizar la predicción más precisa he utilizado distintos tipos de algoritmos para así tener el mejor modelo entrenado que me interese en función de la mejor predicción. Por ejemplo: **Xgboost, Red Neuronal Artificial con distintos optimizadores, Teorema de Bayes, Regresión Logística, Máquina Soporte Vectorial, etc.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c445d",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852b26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c50240",
   "metadata": {},
   "source": [
    "### Clase Algoritmos\n",
    "Esta clase se encargará de calcular la precisión de los algoritmos siempre y cuando inicialice correctamente el constructor con sus parametros correctos. Después solo hay que llamar a la funcion **precision()**. También almacenará los datos de precisión en un dataframe **df** para una consulta posterior en el apartado de la conclusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09613a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = {'Nombre_Algoritmo':[], 'Precision':[]}  \n",
    "df = pd.DataFrame(data=datos) \n",
    "class Algoritmos:\n",
    "    \n",
    "    def __init__(self,nombre,cm, y_test):\n",
    "        self.nombre = nombre\n",
    "        self.cm = cm\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (self.nombre)\n",
    "        \n",
    "    def precision(self):\n",
    "        global df\n",
    "        assert(self.cm.shape==(2,2))\n",
    "        assert(type(self.y_test) == pd.DataFrame)\n",
    "        assert(type(self.cm[0][0]) == np.int64)\n",
    "        precision_02 = ((self.cm[0][0] + self.cm[1][1])/len(self.y_test)) * 100\n",
    "        assert(type(precision_02) == np.float64)\n",
    "        df = df.append({'Nombre_Algoritmo':self.nombre, 'Precision':precision_02}, ignore_index=True)\n",
    "        return(\"%.2f\" %precision_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae187564",
   "metadata": {},
   "source": [
    "###  Carga y Visualización de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9184e7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH\n",
    "path = \"C:/Users/USUARIO/Desktop/BancarrotaEmpresas/\"\n",
    "data = pd.read_csv(path + \"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63d55289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "      <td>0.035490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.543230</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.604462</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797409</td>\n",
       "      <td>0.809331</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.623620</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.279606</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.566193</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0</td>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.524172</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809327</td>\n",
       "      <td>0.303520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0</td>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.610441</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0</td>\n",
       "      <td>0.506264</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.554045</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.809399</td>\n",
       "      <td>0.303498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.841084</td>\n",
       "      <td>0.277547</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>1</td>\n",
       "      <td>0.044009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0</td>\n",
       "      <td>0.493053</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.549548</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.998080</td>\n",
       "      <td>0.801987</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.841019</td>\n",
       "      <td>0.275114</td>\n",
       "      <td>0.026793</td>\n",
       "      <td>0.565167</td>\n",
       "      <td>1</td>\n",
       "      <td>0.233902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "0             1                                           0.370594          \n",
       "1             1                                           0.464291          \n",
       "2             1                                           0.426071          \n",
       "3             1                                           0.399844          \n",
       "4             1                                           0.465022          \n",
       "...         ...                                                ...          \n",
       "6814          0                                           0.493687          \n",
       "6815          0                                           0.475162          \n",
       "6816          0                                           0.472725          \n",
       "6817          0                                           0.506264          \n",
       "6818          0                                           0.493053          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "0                                    0.424389   \n",
       "1                                    0.538214   \n",
       "2                                    0.499019   \n",
       "3                                    0.451265   \n",
       "4                                    0.538432   \n",
       "...                                       ...   \n",
       "6814                                 0.539468   \n",
       "6815                                 0.538269   \n",
       "6816                                 0.533744   \n",
       "6817                                 0.559911   \n",
       "6818                                 0.570105   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "0                                              0.405750    \n",
       "1                                              0.516730    \n",
       "2                                              0.472295    \n",
       "3                                              0.457733    \n",
       "4                                              0.522298    \n",
       "...                                                 ...    \n",
       "6814                                           0.543230    \n",
       "6815                                           0.524172    \n",
       "6816                                           0.520638    \n",
       "6817                                           0.554045    \n",
       "6818                                           0.549548    \n",
       "\n",
       "       Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                    0.601457                      0.601457   \n",
       "1                    0.610235                      0.610235   \n",
       "2                    0.601450                      0.601364   \n",
       "3                    0.583541                      0.583541   \n",
       "4                    0.598783                      0.598783   \n",
       "...                       ...                           ...   \n",
       "6814                 0.604455                      0.604462   \n",
       "6815                 0.598308                      0.598308   \n",
       "6816                 0.610444                      0.610213   \n",
       "6817                 0.607850                      0.607850   \n",
       "6818                 0.627409                      0.627409   \n",
       "\n",
       "       Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                   0.998969                    0.796887   \n",
       "1                   0.998946                    0.797380   \n",
       "2                   0.998857                    0.796403   \n",
       "3                   0.998700                    0.796967   \n",
       "4                   0.998973                    0.797366   \n",
       "...                      ...                         ...   \n",
       "6814                0.998992                    0.797409   \n",
       "6815                0.998992                    0.797414   \n",
       "6816                0.998984                    0.797401   \n",
       "6817                0.999074                    0.797500   \n",
       "6818                0.998080                    0.801987   \n",
       "\n",
       "       After-tax net Interest Rate  \\\n",
       "0                         0.808809   \n",
       "1                         0.809301   \n",
       "2                         0.808388   \n",
       "3                         0.808966   \n",
       "4                         0.809304   \n",
       "...                            ...   \n",
       "6814                      0.809331   \n",
       "6815                      0.809327   \n",
       "6816                      0.809317   \n",
       "6817                      0.809399   \n",
       "6818                      0.813800   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  ...  \\\n",
       "0                                         0.302646  ...   \n",
       "1                                         0.303556  ...   \n",
       "2                                         0.302035  ...   \n",
       "3                                         0.303350  ...   \n",
       "4                                         0.303475  ...   \n",
       "...                                            ...  ...   \n",
       "6814                                      0.303510  ...   \n",
       "6815                                      0.303520  ...   \n",
       "6816                                      0.303512  ...   \n",
       "6817                                      0.303498  ...   \n",
       "6818                                      0.313415  ...   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0                        0.716845                    0.009219   \n",
       "1                        0.795297                    0.008323   \n",
       "2                        0.774670                    0.040003   \n",
       "3                        0.739555                    0.003252   \n",
       "4                        0.795016                    0.003878   \n",
       "...                           ...                         ...   \n",
       "6814                     0.799927                    0.000466   \n",
       "6815                     0.799748                    0.001959   \n",
       "6816                     0.797778                    0.002840   \n",
       "6817                     0.811808                    0.002837   \n",
       "6818                     0.815956                    0.000707   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "0                0.622879                0.601453   \n",
       "1                0.623652                0.610237   \n",
       "2                0.623841                0.601449   \n",
       "3                0.622929                0.583538   \n",
       "4                0.623521                0.598782   \n",
       "...                   ...                     ...   \n",
       "6814             0.623620                0.604455   \n",
       "6815             0.623931                0.598306   \n",
       "6816             0.624156                0.610441   \n",
       "6817             0.623957                0.607846   \n",
       "6818             0.626680                0.627408   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                                0.827890              0.290202   \n",
       "1                                0.839969              0.283846   \n",
       "2                                0.836774              0.290189   \n",
       "3                                0.834697              0.281721   \n",
       "4                                0.839973              0.278514   \n",
       "...                                   ...                   ...   \n",
       "6814                             0.840359              0.279606   \n",
       "6815                             0.840306              0.278132   \n",
       "6816                             0.840138              0.275789   \n",
       "6817                             0.841084              0.277547   \n",
       "6818                             0.841019              0.275114   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "0                                0.026601   \n",
       "1                                0.264577   \n",
       "2                                0.026555   \n",
       "3                                0.026697   \n",
       "4                                0.024752   \n",
       "...                                   ...   \n",
       "6814                             0.027064   \n",
       "6815                             0.027009   \n",
       "6816                             0.026791   \n",
       "6817                             0.026822   \n",
       "6818                             0.026793   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "0                                              0.564050                   1   \n",
       "1                                              0.570175                   1   \n",
       "2                                              0.563706                   1   \n",
       "3                                              0.564663                   1   \n",
       "4                                              0.575617                   1   \n",
       "...                                                 ...                 ...   \n",
       "6814                                           0.566193                   1   \n",
       "6815                                           0.566018                   1   \n",
       "6816                                           0.565158                   1   \n",
       "6817                                           0.565302                   1   \n",
       "6818                                           0.565167                   1   \n",
       "\n",
       "       Equity to Liability  \n",
       "0                 0.016469  \n",
       "1                 0.020794  \n",
       "2                 0.016474  \n",
       "3                 0.023982  \n",
       "4                 0.035490  \n",
       "...                    ...  \n",
       "6814              0.029890  \n",
       "6815              0.038284  \n",
       "6816              0.097649  \n",
       "6817              0.044009  \n",
       "6818              0.233902  \n",
       "\n",
       "[6819 rows x 96 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # 6819 entrados de datos con 96 columnas cada una de ellas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e1e284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = data['Bankrupt?'] == 1 # Empresas que han sufrido bancarrota\n",
    "filtro = data[p]\n",
    "len(filtro.index) # 220 empresas de las 6819 empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75341f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "      <th>Equity to Liability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6.819000e+03</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.000000</td>\n",
       "      <td>6819.0</td>\n",
       "      <td>6819.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.032263</td>\n",
       "      <td>0.505180</td>\n",
       "      <td>0.558625</td>\n",
       "      <td>0.553589</td>\n",
       "      <td>0.607948</td>\n",
       "      <td>0.607929</td>\n",
       "      <td>0.998755</td>\n",
       "      <td>0.797190</td>\n",
       "      <td>0.809084</td>\n",
       "      <td>0.303623</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807760</td>\n",
       "      <td>1.862942e+07</td>\n",
       "      <td>0.623915</td>\n",
       "      <td>0.607946</td>\n",
       "      <td>0.840402</td>\n",
       "      <td>0.280365</td>\n",
       "      <td>0.027541</td>\n",
       "      <td>0.565358</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.047578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.176710</td>\n",
       "      <td>0.060686</td>\n",
       "      <td>0.065620</td>\n",
       "      <td>0.061595</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.016916</td>\n",
       "      <td>0.013010</td>\n",
       "      <td>0.012869</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.011163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040332</td>\n",
       "      <td>3.764501e+08</td>\n",
       "      <td>0.012290</td>\n",
       "      <td>0.016934</td>\n",
       "      <td>0.014523</td>\n",
       "      <td>0.014463</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.013214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.476527</td>\n",
       "      <td>0.535543</td>\n",
       "      <td>0.527277</td>\n",
       "      <td>0.600445</td>\n",
       "      <td>0.600434</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.797386</td>\n",
       "      <td>0.809312</td>\n",
       "      <td>0.303466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.796750</td>\n",
       "      <td>9.036205e-04</td>\n",
       "      <td>0.623636</td>\n",
       "      <td>0.600443</td>\n",
       "      <td>0.840115</td>\n",
       "      <td>0.276944</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.024477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502706</td>\n",
       "      <td>0.559802</td>\n",
       "      <td>0.552278</td>\n",
       "      <td>0.605997</td>\n",
       "      <td>0.605976</td>\n",
       "      <td>0.999022</td>\n",
       "      <td>0.797464</td>\n",
       "      <td>0.809375</td>\n",
       "      <td>0.303525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810619</td>\n",
       "      <td>2.085213e-03</td>\n",
       "      <td>0.623879</td>\n",
       "      <td>0.605998</td>\n",
       "      <td>0.841179</td>\n",
       "      <td>0.278778</td>\n",
       "      <td>0.026808</td>\n",
       "      <td>0.565252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.535563</td>\n",
       "      <td>0.589157</td>\n",
       "      <td>0.584105</td>\n",
       "      <td>0.613914</td>\n",
       "      <td>0.613842</td>\n",
       "      <td>0.999095</td>\n",
       "      <td>0.797579</td>\n",
       "      <td>0.809469</td>\n",
       "      <td>0.303585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.826455</td>\n",
       "      <td>5.269777e-03</td>\n",
       "      <td>0.624168</td>\n",
       "      <td>0.613913</td>\n",
       "      <td>0.842357</td>\n",
       "      <td>0.281449</td>\n",
       "      <td>0.026913</td>\n",
       "      <td>0.565725</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.052838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.820000e+09</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Bankrupt?   ROA(C) before interest and depreciation before interest  \\\n",
       "count  6819.000000                                        6819.000000          \n",
       "mean      0.032263                                           0.505180          \n",
       "std       0.176710                                           0.060686          \n",
       "min       0.000000                                           0.000000          \n",
       "25%       0.000000                                           0.476527          \n",
       "50%       0.000000                                           0.502706          \n",
       "75%       0.000000                                           0.535563          \n",
       "max       1.000000                                           1.000000          \n",
       "\n",
       "        ROA(A) before interest and % after tax  \\\n",
       "count                              6819.000000   \n",
       "mean                                  0.558625   \n",
       "std                                   0.065620   \n",
       "min                                   0.000000   \n",
       "25%                                   0.535543   \n",
       "50%                                   0.559802   \n",
       "75%                                   0.589157   \n",
       "max                                   1.000000   \n",
       "\n",
       "        ROA(B) before interest and depreciation after tax  \\\n",
       "count                                        6819.000000    \n",
       "mean                                            0.553589    \n",
       "std                                             0.061595    \n",
       "min                                             0.000000    \n",
       "25%                                             0.527277    \n",
       "50%                                             0.552278    \n",
       "75%                                             0.584105    \n",
       "max                                             1.000000    \n",
       "\n",
       "        Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "count              6819.000000                   6819.000000   \n",
       "mean                  0.607948                      0.607929   \n",
       "std                   0.016934                      0.016916   \n",
       "min                   0.000000                      0.000000   \n",
       "25%                   0.600445                      0.600434   \n",
       "50%                   0.605997                      0.605976   \n",
       "75%                   0.613914                      0.613842   \n",
       "max                   1.000000                      1.000000   \n",
       "\n",
       "        Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "count             6819.000000                 6819.000000   \n",
       "mean                 0.998755                    0.797190   \n",
       "std                  0.013010                    0.012869   \n",
       "min                  0.000000                    0.000000   \n",
       "25%                  0.998969                    0.797386   \n",
       "50%                  0.999022                    0.797464   \n",
       "75%                  0.999095                    0.797579   \n",
       "max                  1.000000                    1.000000   \n",
       "\n",
       "        After-tax net Interest Rate  \\\n",
       "count                   6819.000000   \n",
       "mean                       0.809084   \n",
       "std                        0.013601   \n",
       "min                        0.000000   \n",
       "25%                        0.809312   \n",
       "50%                        0.809375   \n",
       "75%                        0.809469   \n",
       "max                        1.000000   \n",
       "\n",
       "        Non-industry income and expenditure/revenue  ...  \\\n",
       "count                                   6819.000000  ...   \n",
       "mean                                       0.303623  ...   \n",
       "std                                        0.011163  ...   \n",
       "min                                        0.000000  ...   \n",
       "25%                                        0.303466  ...   \n",
       "50%                                        0.303525  ...   \n",
       "75%                                        0.303585  ...   \n",
       "max                                        1.000000  ...   \n",
       "\n",
       "        Net Income to Total Assets   Total assets to GNP price  \\\n",
       "count                  6819.000000                6.819000e+03   \n",
       "mean                      0.807760                1.862942e+07   \n",
       "std                       0.040332                3.764501e+08   \n",
       "min                       0.000000                0.000000e+00   \n",
       "25%                       0.796750                9.036205e-04   \n",
       "50%                       0.810619                2.085213e-03   \n",
       "75%                       0.826455                5.269777e-03   \n",
       "max                       1.000000                9.820000e+09   \n",
       "\n",
       "        No-credit Interval   Gross Profit to Sales  \\\n",
       "count          6819.000000             6819.000000   \n",
       "mean              0.623915                0.607946   \n",
       "std               0.012290                0.016934   \n",
       "min               0.000000                0.000000   \n",
       "25%               0.623636                0.600443   \n",
       "50%               0.623879                0.605998   \n",
       "75%               0.624168                0.613913   \n",
       "max               1.000000                1.000000   \n",
       "\n",
       "        Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "count                          6819.000000           6819.000000   \n",
       "mean                              0.840402              0.280365   \n",
       "std                               0.014523              0.014463   \n",
       "min                               0.000000              0.000000   \n",
       "25%                               0.840115              0.276944   \n",
       "50%                               0.841179              0.278778   \n",
       "75%                               0.842357              0.281449   \n",
       "max                               1.000000              1.000000   \n",
       "\n",
       "        Degree of Financial Leverage (DFL)  \\\n",
       "count                          6819.000000   \n",
       "mean                              0.027541   \n",
       "std                               0.015668   \n",
       "min                               0.000000   \n",
       "25%                               0.026791   \n",
       "50%                               0.026808   \n",
       "75%                               0.026913   \n",
       "max                               1.000000   \n",
       "\n",
       "        Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \\\n",
       "count                                        6819.000000              6819.0   \n",
       "mean                                            0.565358                 1.0   \n",
       "std                                             0.013214                 0.0   \n",
       "min                                             0.000000                 1.0   \n",
       "25%                                             0.565158                 1.0   \n",
       "50%                                             0.565252                 1.0   \n",
       "75%                                             0.565725                 1.0   \n",
       "max                                             1.000000                 1.0   \n",
       "\n",
       "        Equity to Liability  \n",
       "count           6819.000000  \n",
       "mean               0.047578  \n",
       "std                0.050014  \n",
       "min                0.000000  \n",
       "25%                0.024477  \n",
       "50%                0.033798  \n",
       "75%                0.052838  \n",
       "max                1.000000  \n",
       "\n",
       "[8 rows x 96 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c10019e",
   "metadata": {},
   "source": [
    "### Preprocesado de datos\n",
    "\n",
    "Primero compruebo si hay algun valor nulo y duplicado en mi conjunto de datos. Luego observo el tipo de datos por si existiera alguna columna tipo object para transformarla. En este caso no es necesario ya que todas son entradas de datos tipo float64 e int64. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "774fcca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesado\n",
    "data.duplicated().sum() # ninguno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "562b991e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bankrupt?                                                   0\n",
       " ROA(C) before interest and depreciation before interest    0\n",
       " ROA(A) before interest and % after tax                     0\n",
       " ROA(B) before interest and depreciation after tax          0\n",
       " Operating Gross Margin                                     0\n",
       "                                                           ..\n",
       " Liability to Equity                                        0\n",
       " Degree of Financial Leverage (DFL)                         0\n",
       " Interest Coverage Ratio (Interest expense to EBIT)         0\n",
       " Net Income Flag                                            0\n",
       " Equity to Liability                                        0\n",
       "Length: 96, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # ninguno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ad6b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bankrupt?                                                     int64\n",
       " ROA(C) before interest and depreciation before interest    float64\n",
       " ROA(A) before interest and % after tax                     float64\n",
       " ROA(B) before interest and depreciation after tax          float64\n",
       " Operating Gross Margin                                     float64\n",
       "                                                             ...   \n",
       " Liability to Equity                                        float64\n",
       " Degree of Financial Leverage (DFL)                         float64\n",
       " Interest Coverage Ratio (Interest expense to EBIT)         float64\n",
       " Net Income Flag                                              int64\n",
       " Equity to Liability                                        float64\n",
       "Length: 96, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739190f",
   "metadata": {},
   "source": [
    "### Transformacíon de datos\n",
    "\n",
    "Almaceno las columnas que considero variables independientes y dependiente, por ejemplo en este supuesto todas las columnas son importantes en el problema de modo que no me deshago de ninguna. El siguiente paso es considerar quien es la variable dependiente y las variables independientes:\n",
    "\n",
    "- El objetivo de este supesto es predecir la columna **'Bankrupt?'** para pronosticar si una empresa entrará en bancarrota o no . De modo que la columna **'Bankrupt?' será la variable dependiente**.\n",
    "\n",
    "- Como he dicho antes, todas las columnas tienen relevancia en este problema entonces **las variables independientes seran todas las columnas menos 'Bankrupt'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4040e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:-1]\n",
    "Y = data.iloc[:, 0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "688b4859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROA(C) before interest and depreciation before interest</th>\n",
       "      <th>ROA(A) before interest and % after tax</th>\n",
       "      <th>ROA(B) before interest and depreciation after tax</th>\n",
       "      <th>Operating Gross Margin</th>\n",
       "      <th>Realized Sales Gross Margin</th>\n",
       "      <th>Operating Profit Rate</th>\n",
       "      <th>Pre-tax net Interest Rate</th>\n",
       "      <th>After-tax net Interest Rate</th>\n",
       "      <th>Non-industry income and expenditure/revenue</th>\n",
       "      <th>Continuous interest rate (after tax)</th>\n",
       "      <th>...</th>\n",
       "      <th>Liability-Assets Flag</th>\n",
       "      <th>Net Income to Total Assets</th>\n",
       "      <th>Total assets to GNP price</th>\n",
       "      <th>No-credit Interval</th>\n",
       "      <th>Gross Profit to Sales</th>\n",
       "      <th>Net Income to Stockholder's Equity</th>\n",
       "      <th>Liability to Equity</th>\n",
       "      <th>Degree of Financial Leverage (DFL)</th>\n",
       "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
       "      <th>Net Income Flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.370594</td>\n",
       "      <td>0.424389</td>\n",
       "      <td>0.405750</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.601457</td>\n",
       "      <td>0.998969</td>\n",
       "      <td>0.796887</td>\n",
       "      <td>0.808809</td>\n",
       "      <td>0.302646</td>\n",
       "      <td>0.780985</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.716845</td>\n",
       "      <td>0.009219</td>\n",
       "      <td>0.622879</td>\n",
       "      <td>0.601453</td>\n",
       "      <td>0.827890</td>\n",
       "      <td>0.290202</td>\n",
       "      <td>0.026601</td>\n",
       "      <td>0.564050</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.464291</td>\n",
       "      <td>0.538214</td>\n",
       "      <td>0.516730</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.610235</td>\n",
       "      <td>0.998946</td>\n",
       "      <td>0.797380</td>\n",
       "      <td>0.809301</td>\n",
       "      <td>0.303556</td>\n",
       "      <td>0.781506</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795297</td>\n",
       "      <td>0.008323</td>\n",
       "      <td>0.623652</td>\n",
       "      <td>0.610237</td>\n",
       "      <td>0.839969</td>\n",
       "      <td>0.283846</td>\n",
       "      <td>0.264577</td>\n",
       "      <td>0.570175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.426071</td>\n",
       "      <td>0.499019</td>\n",
       "      <td>0.472295</td>\n",
       "      <td>0.601450</td>\n",
       "      <td>0.601364</td>\n",
       "      <td>0.998857</td>\n",
       "      <td>0.796403</td>\n",
       "      <td>0.808388</td>\n",
       "      <td>0.302035</td>\n",
       "      <td>0.780284</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774670</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.623841</td>\n",
       "      <td>0.601449</td>\n",
       "      <td>0.836774</td>\n",
       "      <td>0.290189</td>\n",
       "      <td>0.026555</td>\n",
       "      <td>0.563706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.399844</td>\n",
       "      <td>0.451265</td>\n",
       "      <td>0.457733</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.583541</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.796967</td>\n",
       "      <td>0.808966</td>\n",
       "      <td>0.303350</td>\n",
       "      <td>0.781241</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.739555</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.622929</td>\n",
       "      <td>0.583538</td>\n",
       "      <td>0.834697</td>\n",
       "      <td>0.281721</td>\n",
       "      <td>0.026697</td>\n",
       "      <td>0.564663</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.465022</td>\n",
       "      <td>0.538432</td>\n",
       "      <td>0.522298</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.598783</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.797366</td>\n",
       "      <td>0.809304</td>\n",
       "      <td>0.303475</td>\n",
       "      <td>0.781550</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.795016</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.623521</td>\n",
       "      <td>0.598782</td>\n",
       "      <td>0.839973</td>\n",
       "      <td>0.278514</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.575617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.539468</td>\n",
       "      <td>0.543230</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.604462</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797409</td>\n",
       "      <td>0.809331</td>\n",
       "      <td>0.303510</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799927</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.623620</td>\n",
       "      <td>0.604455</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>0.279606</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.566193</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.538269</td>\n",
       "      <td>0.524172</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.598308</td>\n",
       "      <td>0.998992</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.809327</td>\n",
       "      <td>0.303520</td>\n",
       "      <td>0.781586</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799748</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.598306</td>\n",
       "      <td>0.840306</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>0.027009</td>\n",
       "      <td>0.566018</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0.472725</td>\n",
       "      <td>0.533744</td>\n",
       "      <td>0.520638</td>\n",
       "      <td>0.610444</td>\n",
       "      <td>0.610213</td>\n",
       "      <td>0.998984</td>\n",
       "      <td>0.797401</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.303512</td>\n",
       "      <td>0.781546</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.002840</td>\n",
       "      <td>0.624156</td>\n",
       "      <td>0.610441</td>\n",
       "      <td>0.840138</td>\n",
       "      <td>0.275789</td>\n",
       "      <td>0.026791</td>\n",
       "      <td>0.565158</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0.506264</td>\n",
       "      <td>0.559911</td>\n",
       "      <td>0.554045</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.607850</td>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.797500</td>\n",
       "      <td>0.809399</td>\n",
       "      <td>0.303498</td>\n",
       "      <td>0.781663</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.811808</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.623957</td>\n",
       "      <td>0.607846</td>\n",
       "      <td>0.841084</td>\n",
       "      <td>0.277547</td>\n",
       "      <td>0.026822</td>\n",
       "      <td>0.565302</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0.493053</td>\n",
       "      <td>0.570105</td>\n",
       "      <td>0.549548</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.627409</td>\n",
       "      <td>0.998080</td>\n",
       "      <td>0.801987</td>\n",
       "      <td>0.813800</td>\n",
       "      <td>0.313415</td>\n",
       "      <td>0.786079</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.815956</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.626680</td>\n",
       "      <td>0.627408</td>\n",
       "      <td>0.841019</td>\n",
       "      <td>0.275114</td>\n",
       "      <td>0.026793</td>\n",
       "      <td>0.565167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 94 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ROA(C) before interest and depreciation before interest  \\\n",
       "0                                              0.370594          \n",
       "1                                              0.464291          \n",
       "2                                              0.426071          \n",
       "3                                              0.399844          \n",
       "4                                              0.465022          \n",
       "...                                                 ...          \n",
       "6814                                           0.493687          \n",
       "6815                                           0.475162          \n",
       "6816                                           0.472725          \n",
       "6817                                           0.506264          \n",
       "6818                                           0.493053          \n",
       "\n",
       "       ROA(A) before interest and % after tax  \\\n",
       "0                                    0.424389   \n",
       "1                                    0.538214   \n",
       "2                                    0.499019   \n",
       "3                                    0.451265   \n",
       "4                                    0.538432   \n",
       "...                                       ...   \n",
       "6814                                 0.539468   \n",
       "6815                                 0.538269   \n",
       "6816                                 0.533744   \n",
       "6817                                 0.559911   \n",
       "6818                                 0.570105   \n",
       "\n",
       "       ROA(B) before interest and depreciation after tax  \\\n",
       "0                                              0.405750    \n",
       "1                                              0.516730    \n",
       "2                                              0.472295    \n",
       "3                                              0.457733    \n",
       "4                                              0.522298    \n",
       "...                                                 ...    \n",
       "6814                                           0.543230    \n",
       "6815                                           0.524172    \n",
       "6816                                           0.520638    \n",
       "6817                                           0.554045    \n",
       "6818                                           0.549548    \n",
       "\n",
       "       Operating Gross Margin   Realized Sales Gross Margin  \\\n",
       "0                    0.601457                      0.601457   \n",
       "1                    0.610235                      0.610235   \n",
       "2                    0.601450                      0.601364   \n",
       "3                    0.583541                      0.583541   \n",
       "4                    0.598783                      0.598783   \n",
       "...                       ...                           ...   \n",
       "6814                 0.604455                      0.604462   \n",
       "6815                 0.598308                      0.598308   \n",
       "6816                 0.610444                      0.610213   \n",
       "6817                 0.607850                      0.607850   \n",
       "6818                 0.627409                      0.627409   \n",
       "\n",
       "       Operating Profit Rate   Pre-tax net Interest Rate  \\\n",
       "0                   0.998969                    0.796887   \n",
       "1                   0.998946                    0.797380   \n",
       "2                   0.998857                    0.796403   \n",
       "3                   0.998700                    0.796967   \n",
       "4                   0.998973                    0.797366   \n",
       "...                      ...                         ...   \n",
       "6814                0.998992                    0.797409   \n",
       "6815                0.998992                    0.797414   \n",
       "6816                0.998984                    0.797401   \n",
       "6817                0.999074                    0.797500   \n",
       "6818                0.998080                    0.801987   \n",
       "\n",
       "       After-tax net Interest Rate  \\\n",
       "0                         0.808809   \n",
       "1                         0.809301   \n",
       "2                         0.808388   \n",
       "3                         0.808966   \n",
       "4                         0.809304   \n",
       "...                            ...   \n",
       "6814                      0.809331   \n",
       "6815                      0.809327   \n",
       "6816                      0.809317   \n",
       "6817                      0.809399   \n",
       "6818                      0.813800   \n",
       "\n",
       "       Non-industry income and expenditure/revenue  \\\n",
       "0                                         0.302646   \n",
       "1                                         0.303556   \n",
       "2                                         0.302035   \n",
       "3                                         0.303350   \n",
       "4                                         0.303475   \n",
       "...                                            ...   \n",
       "6814                                      0.303510   \n",
       "6815                                      0.303520   \n",
       "6816                                      0.303512   \n",
       "6817                                      0.303498   \n",
       "6818                                      0.313415   \n",
       "\n",
       "       Continuous interest rate (after tax)  ...   Liability-Assets Flag  \\\n",
       "0                                  0.780985  ...                       0   \n",
       "1                                  0.781506  ...                       0   \n",
       "2                                  0.780284  ...                       0   \n",
       "3                                  0.781241  ...                       0   \n",
       "4                                  0.781550  ...                       0   \n",
       "...                                     ...  ...                     ...   \n",
       "6814                               0.781588  ...                       0   \n",
       "6815                               0.781586  ...                       0   \n",
       "6816                               0.781546  ...                       0   \n",
       "6817                               0.781663  ...                       0   \n",
       "6818                               0.786079  ...                       0   \n",
       "\n",
       "       Net Income to Total Assets   Total assets to GNP price  \\\n",
       "0                        0.716845                    0.009219   \n",
       "1                        0.795297                    0.008323   \n",
       "2                        0.774670                    0.040003   \n",
       "3                        0.739555                    0.003252   \n",
       "4                        0.795016                    0.003878   \n",
       "...                           ...                         ...   \n",
       "6814                     0.799927                    0.000466   \n",
       "6815                     0.799748                    0.001959   \n",
       "6816                     0.797778                    0.002840   \n",
       "6817                     0.811808                    0.002837   \n",
       "6818                     0.815956                    0.000707   \n",
       "\n",
       "       No-credit Interval   Gross Profit to Sales  \\\n",
       "0                0.622879                0.601453   \n",
       "1                0.623652                0.610237   \n",
       "2                0.623841                0.601449   \n",
       "3                0.622929                0.583538   \n",
       "4                0.623521                0.598782   \n",
       "...                   ...                     ...   \n",
       "6814             0.623620                0.604455   \n",
       "6815             0.623931                0.598306   \n",
       "6816             0.624156                0.610441   \n",
       "6817             0.623957                0.607846   \n",
       "6818             0.626680                0.627408   \n",
       "\n",
       "       Net Income to Stockholder's Equity   Liability to Equity  \\\n",
       "0                                0.827890              0.290202   \n",
       "1                                0.839969              0.283846   \n",
       "2                                0.836774              0.290189   \n",
       "3                                0.834697              0.281721   \n",
       "4                                0.839973              0.278514   \n",
       "...                                   ...                   ...   \n",
       "6814                             0.840359              0.279606   \n",
       "6815                             0.840306              0.278132   \n",
       "6816                             0.840138              0.275789   \n",
       "6817                             0.841084              0.277547   \n",
       "6818                             0.841019              0.275114   \n",
       "\n",
       "       Degree of Financial Leverage (DFL)  \\\n",
       "0                                0.026601   \n",
       "1                                0.264577   \n",
       "2                                0.026555   \n",
       "3                                0.026697   \n",
       "4                                0.024752   \n",
       "...                                   ...   \n",
       "6814                             0.027064   \n",
       "6815                             0.027009   \n",
       "6816                             0.026791   \n",
       "6817                             0.026822   \n",
       "6818                             0.026793   \n",
       "\n",
       "       Interest Coverage Ratio (Interest expense to EBIT)   Net Income Flag  \n",
       "0                                              0.564050                   1  \n",
       "1                                              0.570175                   1  \n",
       "2                                              0.563706                   1  \n",
       "3                                              0.564663                   1  \n",
       "4                                              0.575617                   1  \n",
       "...                                                 ...                 ...  \n",
       "6814                                           0.566193                   1  \n",
       "6815                                           0.566018                   1  \n",
       "6816                                           0.565158                   1  \n",
       "6817                                           0.565302                   1  \n",
       "6818                                           0.565167                   1  \n",
       "\n",
       "[6819 rows x 94 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8d7fee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bankrupt?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6814</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6815</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6816</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6818</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6819 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bankrupt?\n",
       "0             1\n",
       "1             1\n",
       "2             1\n",
       "3             1\n",
       "4             1\n",
       "...         ...\n",
       "6814          0\n",
       "6815          0\n",
       "6816          0\n",
       "6817          0\n",
       "6818          0\n",
       "\n",
       "[6819 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d755592",
   "metadata": {},
   "source": [
    "### TRAIN Y TEST\n",
    "Después de visualizar los datos, preprocesarlos, considerar que columnas son relevantes para el caso de estudio y dividirlos en dos matrices ahora ya se pasaría a la fase de división de datos, es decir, considerar cuanto porcentaje de datos del dataset sería para entrenar el modelo, otro porcentaje para testearlo y verificar su precisión. Normalmente:\n",
    "\n",
    "- **TRAIN**, tiene el **80% de datos del dataset original** ya que cuanto más datos más comprobaciones realizará y más fiable será.\n",
    "- **TEST**, tiene el **20% de datos del dataset original** y con él se mide la precisión.\n",
    "\n",
    "Como sería un proceso muy laborioso hacerlo manualmente con está gran cantidad de datos hay una función en Python del modulo **sklearn** llamada **train_test_split** que nos facilita esta división de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f9b6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=(0))\n",
    "\n",
    "# Escalado de variables\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# test 1364 de empresas\n",
    "# train 5455 de empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e9996b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = y_test['Bankrupt?'] == 1 # Empresas que han sufrido bancarrota del test\n",
    "filtro = y_test[p]\n",
    "len(filtro.index) # 46 empresas en bancarrota de las 1364 empresas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74827e8e",
   "metadata": {},
   "source": [
    "### Análisis discriminante lineal\n",
    "El análisis discriminante lineal (Linear Discriminant Analysis o LDA) es un método alternativo más adecuado a la regresión logística cuando la variable cualitativa tiene más de dos niveles (K ≥ 2). Supone también un modelo más estable cuando el tamaño muestral n es pequeño y la distribución de los predictores es aproximadamente normal en cada una de sus clases. El propósito del LDA es encontrar la combinación lineal de las variables originales que permita la mejor separación entre grupos de un set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109b9fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1304,   14],\n",
       "       [  30,   16]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lda.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) #  96.77%  de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cac3138d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b34b30d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.77'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = Algoritmos(\"Análisis discriminante lineal\", cm, y_test)\n",
    "cl.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c338eb",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "Un clasificador AdaBoost es un metaestimador que comienza ajustando un clasificador en el conjunto de datos original y luego ajusta copias adicionales del clasificador en el mismo conjunto de datos, pero donde los pesos de las instancias clasificadas incorrectamente se ajustan de modo que los clasificadores posteriores se centren más en casos difícile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06fe9d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1302,   16],\n",
       "       [  33,   13]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = AdaBoostClassifier(random_state=0)\n",
    "ad.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ad.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred) #  96.41%  de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c730aa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfc55d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.41'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ada = Algoritmos(\"Adaboost\", cm, y_test)\n",
    "ada.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72ab5bc",
   "metadata": {},
   "source": [
    "### Algoritmo Regresión Logística\n",
    "La regresión logística es un tipo de análisis de regresión utilizado para predecir el resultado de una variable categórica (una variable que puede adoptar un número limitado de categorías) en función de las variables independientes o predictoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6fd854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1311,    7],\n",
       "       [  39,    7]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticR = LogisticRegression(random_state=0) # random state = 0\n",
    "LogisticR.fit(X_train, y_train) #Ajustamos el modelo\n",
    "\n",
    "# predicción de los resultados con el conjunto de testing\n",
    "y_pred = LogisticR.predict(X_test)\n",
    "\n",
    "# Matriz de confusion\n",
    "cm = confusion_matrix(y_test, y_pred) \n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "402f9da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a14113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.63'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Reg = Algoritmos(\"Regresión Logística\", cm, y_test)\n",
    "Reg.precision() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3bd39",
   "metadata": {},
   "source": [
    "### Algoritmo KNN (K Vecinos más cercanos)\n",
    "\n",
    "Este algoritmo es un clasificador de aprendizaje supervisado no paramétrico, que utiliza la proximidad para hacer clasificaciones o predicciones sobre la agrupación de un punto de datos individual. Si bien se puede usar para problemas de regresión o clasificación, generalmente se usa como un algoritmo de clasificación, partiendo de la suposición de que se pueden encontrar puntos similares cerca uno del otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60337143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1310,    8],\n",
       "       [  36,   10]], dtype=int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2) # Objeto clasifier\n",
    "KNN.fit(X_train, y_train)\n",
    "\n",
    "y_pred = KNN.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) #  99.95% de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c10ae809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b793ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.77'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = Algoritmos(\"KNV\", cm, y_test) \n",
    "KNN.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5019433",
   "metadata": {},
   "source": [
    "### Algoritmo Teorema de Bayes\n",
    "El teorema de Bayes es utilizado para calcular la probabilidad de un suceso, teniendo información de antemano sobre ese suceso. Podemos calcular la probabilidad de un suceso A, sabiendo además que ese A cumple cierta característica que condiciona su probabilidad. El teorema de Bayes entiende la probabilidad de forma inversa al teorema de la probabilidad total. El teorema de la probabilidad total hace inferencia sobre un suceso B, a partir de los resultados de los sucesos A. Por su parte, Bayes calcula la probabilidad de A condicionado a B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3807f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[324, 994],\n",
       "       [  5,  41]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BAYES = GaussianNB()\n",
    "BAYES.fit(X_train, y_train)\n",
    "\n",
    "y_pred = BAYES.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) #97.82 % de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9c44814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b08d296e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'26.76'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TeorBayes = Algoritmos(\"Teorema de Bayes\", cm, y_test)\n",
    "TeorBayes.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e99a37",
   "metadata": {},
   "source": [
    "### Árbol de Clasificación\n",
    "Un árbol de clasificación es un tipo de árbol de decisiones. Utiliza la medida de impurezas de Gini para clasificar los registros en las categorías del campo objetivo. Las predicciones se basan en combinaciones de valores en los campos de entrada.\n",
    "Un árbol de clasificación calcula la categoría de destino pronosticada para cada nodo en el árbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3116cbd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1288,   30],\n",
       "       [  31,   15]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DecisionTree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "DecisionTree.fit(X_train, y_train)\n",
    "\n",
    "y_pred = DecisionTree.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)  # 99,92% de precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a903ab9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "797897f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'95.53'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Arbol = Algoritmos(\"Árboles de Clasificación\", cm, y_test)\n",
    "Arbol.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d937bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1309,    9],\n",
       "       [  41,    5]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aleatorio Entropia\n",
    "rand = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "rand.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rand.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) # 99.95% precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2641dadc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5aa3d528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.33'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArbolRandomEntropia = Algoritmos(\"Árboles Aleatorios Entropia\", cm, y_test)\n",
    "ArbolRandomEntropia.precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "321502b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1312,    6],\n",
       "       [  35,   11]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aleatorio Gini\n",
    "rand02 = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = 0)\n",
    "rand02.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rand02.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) # 99.94%% precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89b88307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4d01aa82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.99'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ArbolRandomGini = Algoritmos(\"Árboles Aleatorios Gini\", cm, y_test)\n",
    "ArbolRandomGini.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529ef37",
   "metadata": {},
   "source": [
    "### Algoritmo de XGBOOST\n",
    "XGBoost es una biblioteca de aumento de gradiente distribuida optimizada diseñada para ser altamente eficiente , flexible y portátil . Implementa algoritmos de aprendizaje automático bajo el marco Gradient Boosting . XGBoost proporciona un impulso de árbol paralelo (también conocido como GBDT, GBM) que resuelve muchos problemas de ciencia de datos de una manera rápida y precisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea63b8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1303,   15],\n",
       "       [  36,   10]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XGBOOST = XGBClassifier()\n",
    "XGBOOST.fit(X_train, y_train)\n",
    "\n",
    "y_pred = XGBOOST.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred) # % precision\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "27172dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e560824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'96.26'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb = Algoritmos(\"XGBOOST\", cm, y_test)\n",
    "xgb.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05701c00",
   "metadata": {},
   "source": [
    "### Algoritmo Red Neuronal Artificial\n",
    "Una red neuronal es un modelo simplificado que emula el modo en que el cerebro humano procesa la información: Funciona simultaneando un número elevado de unidades de procesamiento interconectadas que parecen versiones abstractas de neuronas.\n",
    "La red aprende examinando los registros individuales, generando una predicción para cada registro y realizando ajustes a las ponderaciones cuando realiza una predicción incorrecta. Este proceso se repite muchas veces y la red sigue mejorando sus predicciones hasta haber alcanzado uno o varios criterios de parada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bb106ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 3ms/step - loss: 0.7654 - accuracy: 0.3182\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7638 - accuracy: 0.3232\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7622 - accuracy: 0.3274\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7607 - accuracy: 0.3335\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7590 - accuracy: 0.3384\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7574 - accuracy: 0.3412\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7558 - accuracy: 0.3470\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7541 - accuracy: 0.3531\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7525 - accuracy: 0.3562\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7507 - accuracy: 0.3628\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7490 - accuracy: 0.3674\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7473 - accuracy: 0.3712\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7456 - accuracy: 0.3765\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7438 - accuracy: 0.3835\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7421 - accuracy: 0.3879\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7404 - accuracy: 0.3918\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7386 - accuracy: 0.3967\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7368 - accuracy: 0.4024\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7350 - accuracy: 0.4086\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.7333 - accuracy: 0.4148\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7315 - accuracy: 0.4189\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.7297 - accuracy: 0.4253\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7279 - accuracy: 0.4319\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7261 - accuracy: 0.4383\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7243 - accuracy: 0.4431\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7225 - accuracy: 0.4482\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 1s 7ms/step - loss: 0.7207 - accuracy: 0.4550\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.7189 - accuracy: 0.4610\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.7170 - accuracy: 0.4664\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7152 - accuracy: 0.4731\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7134 - accuracy: 0.4819\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7116 - accuracy: 0.4884\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7098 - accuracy: 0.4948\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7080 - accuracy: 0.5012\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7062 - accuracy: 0.5098\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7044 - accuracy: 0.5151\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7027 - accuracy: 0.5219\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.7009 - accuracy: 0.5280\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6991 - accuracy: 0.5355\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6973 - accuracy: 0.5428\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6955 - accuracy: 0.5492\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6937 - accuracy: 0.5577\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6919 - accuracy: 0.5643\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.5716\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6883 - accuracy: 0.5765\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.5833\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6847 - accuracy: 0.5888\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6829 - accuracy: 0.5940\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6811 - accuracy: 0.6002\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6794 - accuracy: 0.6070\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6776 - accuracy: 0.6141\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6758 - accuracy: 0.6187\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6740 - accuracy: 0.6235\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6722 - accuracy: 0.6310\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.6372\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6687 - accuracy: 0.6436\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6670 - accuracy: 0.6500\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6652 - accuracy: 0.6544\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6635 - accuracy: 0.6598\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6617 - accuracy: 0.6660\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6600 - accuracy: 0.6728\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6583 - accuracy: 0.6785\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6565 - accuracy: 0.6827\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6548 - accuracy: 0.6863\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6530 - accuracy: 0.6918\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6513 - accuracy: 0.6977\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6496 - accuracy: 0.7021\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6478 - accuracy: 0.7069\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6461 - accuracy: 0.7118\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6444 - accuracy: 0.7173\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6427 - accuracy: 0.7234\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6410 - accuracy: 0.7291\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6393 - accuracy: 0.7329\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.7377\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6358 - accuracy: 0.7426\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.7468\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6324 - accuracy: 0.7501\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6307 - accuracy: 0.7566\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6290 - accuracy: 0.7608\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6273 - accuracy: 0.7650\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6256 - accuracy: 0.7690\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6239 - accuracy: 0.7729\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6222 - accuracy: 0.7762\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6205 - accuracy: 0.7806\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6188 - accuracy: 0.7830\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6171 - accuracy: 0.7861\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.7901\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6138 - accuracy: 0.7936\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6121 - accuracy: 0.7982\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6104 - accuracy: 0.8007\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6088 - accuracy: 0.8042\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6071 - accuracy: 0.8082\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6055 - accuracy: 0.8132\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6038 - accuracy: 0.8169\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6022 - accuracy: 0.8189\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6005 - accuracy: 0.8220\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5989 - accuracy: 0.8244\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.5972 - accuracy: 0.8277\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.5956 - accuracy: 0.8304\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.5940 - accuracy: 0.8334\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 3ms/step - loss: 0.5513 - accuracy: 0.8940\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.9349\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.9498\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.4439 - accuracy: 0.9567\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.4235 - accuracy: 0.9608\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.4061 - accuracy: 0.9630\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.9635\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.9637\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.9648\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 0.9650\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.3429 - accuracy: 0.9657\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.3333 - accuracy: 0.9659\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.9659\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.9661\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.3083 - accuracy: 0.9665\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.3010 - accuracy: 0.9665\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.9666\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2876 - accuracy: 0.9668\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2815 - accuracy: 0.9672\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2756 - accuracy: 0.9672\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2701 - accuracy: 0.9672\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2648 - accuracy: 0.9674\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2598 - accuracy: 0.9674\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.9674\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.9674\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2461 - accuracy: 0.9676\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2419 - accuracy: 0.9677\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2380 - accuracy: 0.9677\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2342 - accuracy: 0.9677\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2305 - accuracy: 0.9677\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2270 - accuracy: 0.9677\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2237 - accuracy: 0.9677\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2205 - accuracy: 0.9677\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2174 - accuracy: 0.9677\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9677\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2115 - accuracy: 0.9677\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2087 - accuracy: 0.9677\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9677\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9677\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9677\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1985 - accuracy: 0.9677\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1962 - accuracy: 0.9677\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1939 - accuracy: 0.9677\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1917 - accuracy: 0.9677\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9677\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1875 - accuracy: 0.9677\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1855 - accuracy: 0.9677\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1835 - accuracy: 0.9677\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1816 - accuracy: 0.9677\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9677\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1781 - accuracy: 0.9677\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1763 - accuracy: 0.9677\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1747 - accuracy: 0.9677\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1730 - accuracy: 0.9677\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9677\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1699 - accuracy: 0.9677\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1685 - accuracy: 0.9677\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1670 - accuracy: 0.9677\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1656 - accuracy: 0.9677\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1643 - accuracy: 0.9677\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1629 - accuracy: 0.9679\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1616 - accuracy: 0.9679\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1604 - accuracy: 0.9681\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1592 - accuracy: 0.9681\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1580 - accuracy: 0.9681\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1568 - accuracy: 0.9681\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1557 - accuracy: 0.9681\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1546 - accuracy: 0.9681\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1535 - accuracy: 0.9681\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9681\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1514 - accuracy: 0.9681\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1504 - accuracy: 0.9681\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1495 - accuracy: 0.9681\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1485 - accuracy: 0.9681\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9681\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1467 - accuracy: 0.9681\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9679\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1449 - accuracy: 0.9679\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1440 - accuracy: 0.9679\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9679\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1424 - accuracy: 0.9679\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1416 - accuracy: 0.9679\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1408 - accuracy: 0.9679\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.9679\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.1393 - accuracy: 0.9679\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1385 - accuracy: 0.9679\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.1378 - accuracy: 0.9679\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.1371 - accuracy: 0.9679\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.1364 - accuracy: 0.9681\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.1357 - accuracy: 0.9681\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9681\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 0.9681\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.1338 - accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.1332 - accuracy: 0.9681\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.1326 - accuracy: 0.9681\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9681\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1314 - accuracy: 0.9683\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.1308 - accuracy: 0.9683\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.1302 - accuracy: 0.9683\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9683\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 3ms/step - loss: 0.1096 - accuracy: 0.9688\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9688\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9690\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9696\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0785 - accuracy: 0.9712\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9709\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9721\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0726 - accuracy: 0.9727\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9734\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0692 - accuracy: 0.9738\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9736\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9745\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0663 - accuracy: 0.9753\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9756\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0638 - accuracy: 0.9762\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0614 - accuracy: 0.9758\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0603 - accuracy: 0.9762\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0581 - accuracy: 0.9775\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9773\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0560 - accuracy: 0.9791\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0546 - accuracy: 0.9795\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9795\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0521 - accuracy: 0.9797\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0512 - accuracy: 0.9804\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0492 - accuracy: 0.9813\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0488 - accuracy: 0.9808\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.9804\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0460 - accuracy: 0.9830\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0455 - accuracy: 0.9820\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0447 - accuracy: 0.9822\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0441 - accuracy: 0.9835\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0422 - accuracy: 0.9835\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0428 - accuracy: 0.9835\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0398 - accuracy: 0.9831\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0383 - accuracy: 0.9852\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.9859\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.9863\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0353 - accuracy: 0.9863\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.9861\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9870\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.9879\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.9870\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.9883\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.9892\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.9885\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9890\n",
      "Epoch 47/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9901\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9892\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.9901\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.9918\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9919\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.9925\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.9921\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.9925\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.9929\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.9936\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.9936\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9940\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9940\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.9927\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.9916\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.9941\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0153 - accuracy: 0.9952\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0137 - accuracy: 0.9965\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0122 - accuracy: 0.9967\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0131 - accuracy: 0.9962\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9951\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0121 - accuracy: 0.9971\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0110 - accuracy: 0.9971\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0105 - accuracy: 0.9971\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0109 - accuracy: 0.9965\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.0105 - accuracy: 0.9967\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0094 - accuracy: 0.9978\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0095 - accuracy: 0.9978\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0086 - accuracy: 0.9976\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9963\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 0.9976\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0090 - accuracy: 0.9971\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0078 - accuracy: 0.9978\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0103 - accuracy: 0.9973\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0075 - accuracy: 0.9980\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0131 - accuracy: 0.9947\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0138 - accuracy: 0.9951\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9967\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0058 - accuracy: 0.9985\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0053 - accuracy: 0.9985\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 0.9987\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0050 - accuracy: 0.9987\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0039 - accuracy: 0.9987\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0040 - accuracy: 0.9987\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 0.9989\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0035 - accuracy: 0.9993\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9980\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 3ms/step - loss: 0.0055 - accuracy: 0.9985\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0042 - accuracy: 0.9991\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0028 - accuracy: 0.9993\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.9993\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.9993\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.9993\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9993\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.9993\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9993\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 0.9993\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 0.9996\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 0.9998\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9998\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 0.9996\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 9.9021e-04 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 9.9161e-04 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 9.3245e-04 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 9.6186e-04 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 9.1578e-04 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.9599e-04 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.7146e-04 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 9.0108e-04 - accuracy: 0.9998\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.7208e-04 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.4482e-04 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.3027e-04 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.3245e-04 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.1364e-04 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 8.0524e-04 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 7.9768e-04 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 7.9042e-04 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.7826e-04 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 7.7613e-04 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.6489e-04 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.3395e-04 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.3699e-04 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.2524e-04 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 7.3017e-04 - accuracy: 0.9998\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 7.4765e-04 - accuracy: 0.9996\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.0800e-04 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 6.8432e-04 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 6.6937e-04 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 6.6601e-04 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 7.0181e-04 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.7645e-04 - accuracy: 0.9998\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.3659e-04 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.5047e-04 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.2330e-04 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.1651e-04 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.0387e-04 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.0367e-04 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 6.0363e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 5.8842e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 6.0045e-04 - accuracy: 0.9998\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 5.7577e-04 - accuracy: 1.0000\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 3ms/step - loss: 0.6802 - accuracy: 0.9685\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6836 - accuracy: 0.9681\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6805 - accuracy: 0.9681\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 0.9681\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6759 - accuracy: 0.9681\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6740 - accuracy: 0.9681\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6723 - accuracy: 0.9681\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6707 - accuracy: 0.9681\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.9681\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.9681\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.9681\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6653 - accuracy: 0.9681\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6641 - accuracy: 0.9681\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6629 - accuracy: 0.9681\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.6618 - accuracy: 0.9681\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 1s 6ms/step - loss: 0.6608 - accuracy: 0.9681\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.6597 - accuracy: 0.9681\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6587 - accuracy: 0.9681\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6578 - accuracy: 0.9681\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6568 - accuracy: 0.9681\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6559 - accuracy: 0.9681\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6550 - accuracy: 0.9681\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6541 - accuracy: 0.9681\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6533 - accuracy: 0.9681\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6525 - accuracy: 0.9681\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.9681\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6509 - accuracy: 0.9681\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6501 - accuracy: 0.9681\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6493 - accuracy: 0.9681\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6486 - accuracy: 0.9681\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6478 - accuracy: 0.9681\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6471 - accuracy: 0.9681\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6464 - accuracy: 0.9681\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6457 - accuracy: 0.9681\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.9681\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6443 - accuracy: 0.9681\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.9681\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6430 - accuracy: 0.9681\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6424 - accuracy: 0.9681\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6417 - accuracy: 0.9681\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6411 - accuracy: 0.9681\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6405 - accuracy: 0.9681\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6399 - accuracy: 0.9681\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6392 - accuracy: 0.9681\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6386 - accuracy: 0.9681\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6381 - accuracy: 0.9681\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6375 - accuracy: 0.9681\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6369 - accuracy: 0.9681\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6363 - accuracy: 0.9681\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.6358 - accuracy: 0.9681\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6352 - accuracy: 0.9681\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6347 - accuracy: 0.9681\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6341 - accuracy: 0.9681\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6336 - accuracy: 0.9681\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6330 - accuracy: 0.9681\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6325 - accuracy: 0.9681\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6320 - accuracy: 0.9681\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6315 - accuracy: 0.9681\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6309 - accuracy: 0.9681\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6304 - accuracy: 0.9681\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6299 - accuracy: 0.9681\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6294 - accuracy: 0.9681\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6289 - accuracy: 0.9681\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6284 - accuracy: 0.9681\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6280 - accuracy: 0.9681\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6275 - accuracy: 0.9681\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6270 - accuracy: 0.9681\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6265 - accuracy: 0.9681\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6261 - accuracy: 0.9681\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6256 - accuracy: 0.9681\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6251 - accuracy: 0.9681\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6247 - accuracy: 0.9681\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6242 - accuracy: 0.9681\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6238 - accuracy: 0.9681\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6233 - accuracy: 0.9681\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6229 - accuracy: 0.9681\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6224 - accuracy: 0.9681\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6220 - accuracy: 0.9681\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6215 - accuracy: 0.9681\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6211 - accuracy: 0.9681\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6207 - accuracy: 0.9681\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6202 - accuracy: 0.9681\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6198 - accuracy: 0.9681\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6194 - accuracy: 0.9681\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6189 - accuracy: 0.9681\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6185 - accuracy: 0.9681\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6181 - accuracy: 0.9681\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6177 - accuracy: 0.9681\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6172 - accuracy: 0.9681\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6168 - accuracy: 0.9681\n",
      "Epoch 91/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6164 - accuracy: 0.9681\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6159 - accuracy: 0.9681\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6155 - accuracy: 0.9681\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6150 - accuracy: 0.9681\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.6146 - accuracy: 0.9681\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.9681\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6137 - accuracy: 0.9681\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6132 - accuracy: 0.9681\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6127 - accuracy: 0.9681\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.6122 - accuracy: 0.9681\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 2s 3ms/step - loss: 0.5265 - accuracy: 0.9681\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1912 - accuracy: 0.9681\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1238 - accuracy: 0.9681\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.1056 - accuracy: 0.9681\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0985 - accuracy: 0.9681\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9681\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0918 - accuracy: 0.9681\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0897 - accuracy: 0.9681\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0879 - accuracy: 0.9681\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0864 - accuracy: 0.9681\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0851 - accuracy: 0.9681\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0839 - accuracy: 0.9681\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9681\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0818 - accuracy: 0.9681\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0808 - accuracy: 0.9681\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0798 - accuracy: 0.9681\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0790 - accuracy: 0.9681\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9681\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9681\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9681\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9681\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9681\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0738 - accuracy: 0.9681\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9681\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0724 - accuracy: 0.9681\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0716 - accuracy: 0.9681\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0707 - accuracy: 0.9681\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0699 - accuracy: 0.9681\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0691 - accuracy: 0.9681\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9681\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0676 - accuracy: 0.9681\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0670 - accuracy: 0.9681\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0661 - accuracy: 0.9681\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9681\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9681\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0640 - accuracy: 0.9681\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0635 - accuracy: 0.9681\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0629 - accuracy: 0.9681\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0623 - accuracy: 0.9681\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0612 - accuracy: 0.9681\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0608 - accuracy: 0.9681\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9681\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.9681\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0590 - accuracy: 0.9681\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0583 - accuracy: 0.9710\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0576 - accuracy: 0.9758\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0569 - accuracy: 0.9767\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0565 - accuracy: 0.9773\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0558 - accuracy: 0.9771\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9784\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9784\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0539 - accuracy: 0.9786\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0533 - accuracy: 0.9784\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.9806\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0517 - accuracy: 0.9809\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0511 - accuracy: 0.9822\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0502 - accuracy: 0.9811\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0498 - accuracy: 0.9824\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0494 - accuracy: 0.9819\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.9817\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0483 - accuracy: 0.9828\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0479 - accuracy: 0.9831\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.9833\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0465 - accuracy: 0.9826\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0463 - accuracy: 0.9833\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0454 - accuracy: 0.9835\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.9835\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.9846\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0440 - accuracy: 0.9837\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.9842\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.9844\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0416 - accuracy: 0.9855\n",
      "Epoch 73/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0416 - accuracy: 0.9850\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0408 - accuracy: 0.9861\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0402 - accuracy: 0.9853\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0407 - accuracy: 0.9853\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0395 - accuracy: 0.9855\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0390 - accuracy: 0.9859\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0384 - accuracy: 0.9864\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0378 - accuracy: 0.9863\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.9877\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9868\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.9863\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.9879\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.9866\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.9875\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.9888\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.9875\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.9875\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.9877\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9890\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.9897\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.9877\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9885\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.9892\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.9885\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.9892\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.9886\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9899\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.9894\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 3ms/step - loss: 0.0294 - accuracy: 0.9888\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0276 - accuracy: 0.9903\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.9899\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.9897\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.9897\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.9901\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9899\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.9905\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9905\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.9897\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.9905\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.9912\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.9912\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.9908\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9912\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9914\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.9925\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.9908\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9901\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9916\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9914\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.9905\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.9916\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9919\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.9914\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.9916\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9921\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.9912\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9912\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.9918\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.9923\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.9910\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.9918\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9927\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9923\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.9914\n",
      "Epoch 37/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.9921\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9914\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.9916\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.9916\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.9927\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.9914\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.9921\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9923\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.9912\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9925\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.9914\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.9927\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9918\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9925\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.9916\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.9918\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.9932\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9925\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.9919\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9929\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9918\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9929\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.9918\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9921\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9923\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.9925\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.9923\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.9925\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.9918\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.9930\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9923\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9929\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.9923\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.9927\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.9923\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.9921\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.9927\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.9925\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.9927\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.9940\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.9930\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.9934\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.9929\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.9934\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0251 - accuracy: 0.9925\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.9930\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9927\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9923\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.9932\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.9930\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.9945\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.9925\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9936\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.9936\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9929\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 0.9929\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 6ms/step - loss: 0.0190 - accuracy: 0.9940\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.0261 - accuracy: 0.9932\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.0278 - accuracy: 0.9932\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.9941\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.9938\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.9934\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.9938\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9938\n",
      "Epoch 1/100\n",
      "86/86 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9945\n",
      "Epoch 2/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.9925\n",
      "Epoch 3/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9927\n",
      "Epoch 4/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.9943\n",
      "Epoch 5/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9947\n",
      "Epoch 6/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9952\n",
      "Epoch 7/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0160 - accuracy: 0.9952\n",
      "Epoch 8/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9952\n",
      "Epoch 9/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9952\n",
      "Epoch 10/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9954\n",
      "Epoch 11/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9952\n",
      "Epoch 12/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9954\n",
      "Epoch 13/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9952\n",
      "Epoch 14/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9954\n",
      "Epoch 15/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9954\n",
      "Epoch 16/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9952\n",
      "Epoch 17/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9954\n",
      "Epoch 18/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9952\n",
      "Epoch 19/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9952\n",
      "Epoch 20/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9954\n",
      "Epoch 21/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9952\n",
      "Epoch 22/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 23/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9952\n",
      "Epoch 24/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9952\n",
      "Epoch 25/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9954\n",
      "Epoch 26/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9954\n",
      "Epoch 27/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0148 - accuracy: 0.9952\n",
      "Epoch 28/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9952\n",
      "Epoch 29/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9952\n",
      "Epoch 30/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9954\n",
      "Epoch 31/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9952\n",
      "Epoch 32/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 33/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 34/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9954\n",
      "Epoch 35/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 36/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9954\n",
      "Epoch 37/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 38/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9952\n",
      "Epoch 39/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9954\n",
      "Epoch 40/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9952\n",
      "Epoch 41/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9954\n",
      "Epoch 42/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9952\n",
      "Epoch 43/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9954\n",
      "Epoch 44/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9952\n",
      "Epoch 45/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9954\n",
      "Epoch 46/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9952\n",
      "Epoch 47/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9954\n",
      "Epoch 48/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 49/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9954\n",
      "Epoch 50/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9954\n",
      "Epoch 51/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9954\n",
      "Epoch 52/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9958\n",
      "Epoch 53/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 54/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9952\n",
      "Epoch 55/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9956\n",
      "Epoch 56/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 57/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9954\n",
      "Epoch 58/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 59/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 60/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 61/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9958\n",
      "Epoch 62/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9956\n",
      "Epoch 63/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0142 - accuracy: 0.9954\n",
      "Epoch 64/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 65/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 66/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 67/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 68/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 69/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.9954\n",
      "Epoch 70/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 71/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 72/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 73/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 74/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 75/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9958\n",
      "Epoch 76/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0141 - accuracy: 0.9954\n",
      "Epoch 77/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 78/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0141 - accuracy: 0.9956\n",
      "Epoch 79/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9958\n",
      "Epoch 80/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 81/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 82/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0140 - accuracy: 0.9958\n",
      "Epoch 83/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9958\n",
      "Epoch 84/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 85/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 86/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 87/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0140 - accuracy: 0.9956\n",
      "Epoch 88/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 89/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 90/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9958\n",
      "Epoch 91/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9960\n",
      "Epoch 92/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9958\n",
      "Epoch 93/100\n",
      "86/86 [==============================] - 0s 5ms/step - loss: 0.0139 - accuracy: 0.9958\n",
      "Epoch 94/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 95/100\n",
      "86/86 [==============================] - 0s 4ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 96/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9960\n",
      "Epoch 97/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 98/100\n",
      "86/86 [==============================] - 0s 3ms/step - loss: 0.0139 - accuracy: 0.9956\n",
      "Epoch 99/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9958\n",
      "Epoch 100/100\n",
      "86/86 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9958\n"
     ]
    }
   ],
   "source": [
    "# red neuronal artificial con keras del modelo secuencial\n",
    "RNA = tf.keras.models.Sequential()\n",
    "\n",
    "# 1Âº Capa de inputs\n",
    "RNA.add(tf.keras.layers.Dense(units=20, activation='relu'))\n",
    "# 2Âº Capa oculta de 6 nodos\n",
    "RNA.add(tf.keras.layers.Dense(units=10, activation='relu'))\n",
    "# 3º Capa\n",
    "RNA.add(tf.keras.layers.Dense(units=5, activation='relu'))\n",
    "# Ultima capa de salidad, con funcion sigmoide\n",
    "RNA.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "diccionario = {0:'Adadelta',1:'Adagrad',2:'Adam',3:'Adamax',4:'Ftrl',5:'Nadam',6:'RMSprop',7:'SGD'}\n",
    "for optimi in diccionario.values():\n",
    "    RNA.compile(optimizer = optimi, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    RNA.fit(X_train, y_train, batch_size = 64, epochs = 100)\n",
    "    \n",
    "    y_pred = RNA.predict(X_test)\n",
    "    y_pred_02 = (y_pred > 0.5)\n",
    "    y_pred_02 = np.where(y_pred_02=='True',1,y_pred_02)\n",
    "    cm = confusion_matrix(y_test, y_pred_02)    \n",
    "    redneuro = Algoritmos(\"Red Neuronal Artificial Optimizador: \" + str(optimi), cm, y_test)\n",
    "    redneuro.precision()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b61149",
   "metadata": {},
   "source": [
    "###  Conclusión\n",
    "**La mejor precisión** la realiza el algoritmo de **Árboles Aleatorios utilizando como criterio el indice Gini** con un **96,99%**.\n",
    "\n",
    "Teniendo la siguiente **matriz de confusión**:\n",
    "- ([[1312,    6],[  35,   11]] **Ha acertado correctamente que 1312 no entraran en bancarrota y 11 que si, del test de datos que eran 1364 empresas**.\n",
    "\n",
    "**De modo que ya podría utilizar este modelo para analizar y predecir otras empresas con estas mismas características.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6395c933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nombre_Algoritmo</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Árboles Aleatorios Gini</td>\n",
       "      <td>96.994135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Análisis discriminante lineal</td>\n",
       "      <td>96.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNV</td>\n",
       "      <td>96.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: Adagrad</td>\n",
       "      <td>96.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regresión Logística</td>\n",
       "      <td>96.627566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: Ftrl</td>\n",
       "      <td>96.627566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>96.407625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Árboles Aleatorios Entropia</td>\n",
       "      <td>96.334311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>XGBOOST</td>\n",
       "      <td>96.260997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: Nadam</td>\n",
       "      <td>95.894428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: Adam</td>\n",
       "      <td>95.821114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Árboles de Clasificación</td>\n",
       "      <td>95.527859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: Adamax</td>\n",
       "      <td>95.527859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: SGD</td>\n",
       "      <td>95.527859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: RMSprop</td>\n",
       "      <td>95.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Red Neuronal Artificial Optimizador: Adadelta</td>\n",
       "      <td>84.164223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Teorema de Bayes</td>\n",
       "      <td>26.759531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Nombre_Algoritmo  Precision\n",
       "7                         Árboles Aleatorios Gini  96.994135\n",
       "0                   Análisis discriminante lineal  96.774194\n",
       "3                                             KNV  96.774194\n",
       "10   Red Neuronal Artificial Optimizador: Adagrad  96.774194\n",
       "2                             Regresión Logística  96.627566\n",
       "13      Red Neuronal Artificial Optimizador: Ftrl  96.627566\n",
       "1                                        Adaboost  96.407625\n",
       "6                     Árboles Aleatorios Entropia  96.334311\n",
       "8                                         XGBOOST  96.260997\n",
       "14     Red Neuronal Artificial Optimizador: Nadam  95.894428\n",
       "11      Red Neuronal Artificial Optimizador: Adam  95.821114\n",
       "5                        Árboles de Clasificación  95.527859\n",
       "12    Red Neuronal Artificial Optimizador: Adamax  95.527859\n",
       "16       Red Neuronal Artificial Optimizador: SGD  95.527859\n",
       "15   Red Neuronal Artificial Optimizador: RMSprop  95.454545\n",
       "9   Red Neuronal Artificial Optimizador: Adadelta  84.164223\n",
       "4                                Teorema de Bayes  26.759531"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.sort_values(by='Precision',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562a845",
   "metadata": {},
   "source": [
    "**Autor: Carlos Mir Martínez**\n",
    "\n",
    "**Fecha: 20/06/2022**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
